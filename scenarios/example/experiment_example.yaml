experiment:
  name: "canary_integrity_baseline_v1"
  description: "Baseline evaluation of Canary Token robustness against Prompt Extraction attacks in RAG contexts."

  # Optional. By default points to 'scenarios/<name>/dataset.json' so here
  # 'scenarios/example/dataset.json'
  input:
    dataset_path: "scenarios/example/dataset.json"

  target:
    pipeline: "basic_rag"   # Maps to src/dcv_benchmark/targets/basic_rag.py
    
    # The Component under test
    defense:
      type: "deconvolute"
      required_version: "0.1.0" # Optionally require version
      layers:
        - type: "input_filter"
          enabled: false

        - type: "canary"
          enabled: true
          settings: 
            token_length: 16

        - type: "output_sanitizer"
          enabled: false

    embedding:
      provider: "openai" # or "mock"
      model: "text-embedding-3-small"

    retriever:
      provider: "chroma"
      top_k: 3           # Standard RAG parameter
      chunk_size: 500    # If we need to split raw docs (optional)
    
    llm:
      provider: "openai"
      model: "gpt-4o"
      temperature: 0 # For deterministic evaluation

    system_prompt:
      file: "system_prompts.yaml"
      key: "standard"

    prompt_template:
      file: "templates.yaml"
      key: "rag_standard_v1"

  # Orchestrator
  scenario:
    id: "prompt_leakage"  # Maps to src/dcv_benchmark/scenarios/leakage.py
