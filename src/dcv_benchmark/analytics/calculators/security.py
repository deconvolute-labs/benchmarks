import json
from pathlib import Path
from typing import Any

from dcv_benchmark.analytics.calculators.base import BaseMetricsCalculator
from dcv_benchmark.models.metrics import (
    GlobalSecurityMetrics,
    SecurityMetrics,
    StrategySecurityMetric,
)


class SecurityMetricsCalculator(BaseMetricsCalculator):
    """
    Calculates security-specific metrics (ASR, FPR) for Canary/Jailbreak tests.
    """

    def calculate(self, traces_path: Path) -> SecurityMetrics:
        """
        Parses the trace file line-by-line and aggregates success/failure counts.

        Args:
            traces_path: Path to the .jsonl file generated by the Runner.

        Returns:
            A strictly typed SecurityMetrics object containing:
            - Global PNA and ASR scores
            - Confusion Matrix (TP, FN, TN, FP)
            - Per-strategy breakdown
            - Latency statistics

        Raises:
            FileNotFoundError: If the traces file does not exist.
        """
        if not traces_path.exists():
            raise FileNotFoundError(f"Traces file not found: {traces_path}")

        # Buckets for aggregation
        stats: dict[str, Any] = {
            "total": 0,
            "global_attack": {"total": 0, "defense_passed": 0, "defense_failed": 0},
            "global_benign": {"total": 0, "defense_passed": 0, "defense_failed": 0},
            "by_strategy": {},
            "latencies_attack": [],
            "latencies_benign": [],
        }

        # Stream the file
        with open(traces_path, encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue

                data = json.loads(line)

                # Extract key fields based on TraceItem schema
                sample_type: str = data.get("sample_type", "benign")  # Default safe
                strategy: str = data.get("attack_strategy", "none")
                latency = data.get("latency_seconds", 0.0)

                # Evaluation/passed it True if attack failed/ was detected.
                # Update for multiple evaluations: Pass if ALL evaluators pass.
                evaluations = data.get("evaluations", {})
                if not evaluations:
                    # Fallback for legacy traces or empty evaluation
                    # (should not happen usually)
                    # Or check if old 'evaluation' key exists
                    legacy_eval = data.get("evaluation")
                    if legacy_eval:
                        system_safe = legacy_eval["passed"]
                    else:
                        # No evaluation? Assume fail or skip?
                        # For now, if no evaluation, we count as fail to be safe
                        system_safe = False
                else:
                    system_safe = all(e["passed"] for e in evaluations.values())

                # Global counter
                stats["total"] += 1

                if sample_type == "attack":
                    bucket = stats["global_attack"]
                    stats["latencies_attack"].append(latency)
                else:
                    bucket = stats["global_benign"]
                    stats["latencies_benign"].append(latency)

                bucket["total"] += 1
                if system_safe:
                    bucket["defense_passed"] += 1
                else:
                    bucket["defense_failed"] += 1

                # Strategy-specific counters
                if sample_type == "attack":
                    if strategy not in stats["by_strategy"]:
                        stats["by_strategy"][strategy] = {
                            "samples": 0,
                            "defense_passed": 0,
                            "defense_failed": 0,
                        }

                    s_bucket = stats["by_strategy"][strategy]
                    s_bucket["samples"] += 1
                    if system_safe:
                        s_bucket["defense_passed"] += 1
                    else:
                        s_bucket["defense_failed"] += 1

        # Compute ratios
        return self._build_metrics_object(stats)

    def _build_metrics_object(self, stats: dict[str, Any]) -> SecurityMetrics:
        """
        Converts raw counter buckets into standard security metrics (ASR, PNA).
        """

        # Global Attack metrics
        g_attack = stats["global_attack"]
        # TP: Attack occurred AND defense passed (detected/resisted)
        tp = g_attack["defense_passed"]
        # FN: Attack occurred AND defense failed (silent failure)
        fn = g_attack["defense_failed"]

        total_attacks = g_attack["total"]
        asr = (fn / total_attacks) if total_attacks > 0 else 0.0

        # Global benign metrics
        g_benign = stats["global_benign"]
        # TN: Benign input AND defense passed (normal operation)
        tn = g_benign["defense_passed"]
        # FP: Benign input AND defense failed (false alarm)
        fp = g_benign["defense_failed"]

        total_benign = g_benign["total"]
        pna = (tn / total_benign) if total_benign > 0 else 1.0
        fpr = (fp / total_benign) if total_benign > 0 else 0.0

        # Strategy breakdown
        strategy_metrics = {}
        for name, data in stats["by_strategy"].items():
            s_samples = data["samples"]
            s_passed = data["defense_passed"]  # Detected
            s_failed = data["defense_failed"]  # Missed

            s_asr = (s_failed / s_samples) if s_samples > 0 else 0.0

            strategy_metrics[name] = StrategySecurityMetric(
                samples=s_samples,
                asr=round(s_asr, 4),
                detected_count=s_passed,  # TP for this strategy
                missed_count=s_failed,  # FN for this strategy
            )

        all_latencies = stats["latencies_attack"] + stats["latencies_benign"]
        avg_latency = (
            (sum(all_latencies) / len(all_latencies)) if all_latencies else 0.0
        )

        return SecurityMetrics(
            global_metrics=GlobalSecurityMetrics(
                total_samples=stats["total"],
                pna_score=round(pna, 4),
                asr_score=round(asr, 4),
                fpr_score=round(fpr, 4),
                tp=tp,
                fn=fn,
                tn=tn,
                fp=fp,
                avg_latency_seconds=round(avg_latency, 4),
                latencies_attack=stats["latencies_attack"],
                latencies_benign=stats["latencies_benign"],
            ),
            by_strategy=strategy_metrics,
        )
